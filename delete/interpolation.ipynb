{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642075ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK_PATH = r'\\\\Desktop-0331tte\\d\\chiletv-testdata\\track\\none@30FPS'\n",
    "TRACK_CSV = r'..\\data\\testdata_tracks.csv'\n",
    "TRACK_LENGTH = 5\n",
    "TRACK_FRAME_RATE = 30.0\n",
    "TRACK_SAMPLES = 5000\n",
    "\n",
    "# DATA_SHAPE:: 5000, TRACK_LENGTH * TRACK_FRAME_RATE , 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb6390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342900d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in Path(TRACK_PATH).glob('**/*.tracks.json'):\n",
    "    with file.open('r', encoding='utf8') as fp:\n",
    "        obj = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16736aa1",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "It is necesary to obtain the bounding box of the faces present for every frame on all videos. The GPU-backed face detector allows to efficiently process many frames in parallel. Despite this, running the face detector for all frames adds up to a considerable time. Addtionally, the face detector is agnostic to previous results leading to undesirable gaps and jittering. A gap is a short secuence of frames where the face remains undetected before suddenly returning, while jittering presents itself when succesive high-frequency detections result in bounding box coordinates that sharply oscillate.\n",
    "\n",
    "The number of frames to be processed by the face detector and their dimensions affect the duration of a detection operation. An intuitive solution is to reduce both of them as reducing the amount of data being transfered between GPU and principal memories the operation can be sped-up. The *speedup* factor stands for the relative decrease in time spent detecting and its given by the ratio between the video length and the time spent processing it.\n",
    "\n",
    "Lowering the detection frame rate can mitigate the jittering while introducing regular periodic gaps on the bounding box sequence. These gaps can be filled with values that soften the transition between detected points. An interpolation estrategy can be selected to fill missing data while keeping this transition as smooth as possible. The tried estrategies correspond the implementations of spline interpolators for zeroth, first, second and third order, that are already available on the *Python* library *SciPy*.\n",
    "\n",
    "To maintain a reference based on actual data, the high frequency samples require an smoothing operation. This low-pass filter takes the form of a moving window averaging past detections. The window sequence contributions can be adjusted by the relative temporal distance to the sampled item. Three smoothing operations were tested: Uniform weights, Inversely linear weights and Gaussian weights. From these the one with the lowest MSE was chosen. The smoothed sequence was used as reference as it is the closest to an ideal result.\n",
    "\n",
    "> The weights for a window of size $n$ on a sequence of length $m$ are computed as follows for Uniform weights ($W^U$), Inversely linear weights ($W^L$) and Gaussian weights ($W^G$).\n",
    "> \n",
    "> $\\forall i \\in [0, m]$ $\\forall j \\in [0, n]$\n",
    "> \n",
    "> $W^U_{ij}=1$\n",
    "> \n",
    "> $W^L_{ij}=\\frac{1}{|t_{i-j}-t_i| + 1}$\n",
    "> \n",
    "> $W^G_{ij}=\\frac{1}{P \\sqrt{2 \\pi}} e^{-\\frac{1}{2}(\\frac{t_{i-j}-t_i}{P})^2}$\n",
    "> \n",
    "> The weights are normalized by the total sum of the weights. Notice that $W^G$ corresponds to the Gaussian distribution which mean is at instant $t_i$ and a deviation of $P=\\frac{1}{f_r}$ with $fr$ being the frame rate.\n",
    "\n",
    "To find the speedup factors a random sample of 150 videos was selected to be run through the face detector with a frame rate of 30, 15, 6, and 3 FPS. To acomodate batches of 1024 frames, all videos were resized by a factor of 8 due to harware restraints. The found bounding boxes for the 30 FPS videos were selected to further analyse the interpolation and smoothing experiments as this frame rate is the maximum allowed value by the the acquired videos. From these results 5000 random samples of 5 seconds were extracted from a total of 3846 tracks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e80f3bc",
   "metadata": {},
   "source": [
    "## Questions\n",
    "> + Does lowering the frame rate make an improovement on detection time?\n",
    "    + By how much?\n",
    "    + Thresholding the frame size to a maximum value may help too, what dimensions to use as threshold?\n",
    "> + Averaging past detections decrease the jittering?\n",
    "    + Which performs the best?\n",
    "> + Which interpolation strategy is closer to the true detection?\n",
    "    + Which is closer to the softened detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc5c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK_POINTS_FILE = r'../data/video_points.pkl'\n",
    "REFERENCE_FRAME_RATE = 30\n",
    "SAMPLE_LENGTH = 5.0\n",
    "SAMPLE_NUM = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6a0424",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8406692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load track data\n",
    "point_data_src = Path(TRACK_POINTS_FILE)\n",
    "with point_data_src.open('rb') as fp:\n",
    "    data_time, data_bbox, data_tlen = pickle.load(fp)\n",
    "print('Number of tracks:', len(data_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1201e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling tracks\n",
    "np.random.seed(0)\n",
    "\n",
    "# Pick random track and a random position to start a slice\n",
    "track_samples = (np.random.random(SAMPLE_NUM) * len(data_time)).astype(int)\n",
    "track_start = np.random.random(SAMPLE_NUM) * (np.float32(data_tlen)[track_samples] - SAMPLE_LENGTH)\n",
    "\n",
    "test_time = []\n",
    "test_bbox = []\n",
    "\n",
    "for ti, ts in zip(track_samples, track_start):\n",
    "    t_time = []\n",
    "    t_bbox = []\n",
    "    # Add the frames that lie between the picked slice bounds\n",
    "    for i in range(len(data_time[ti])):\n",
    "        if ts <= data_time[ti][i] < ts + SAMPLE_LENGTH:\n",
    "            t_time.append(data_time[ti][i])\n",
    "            t_bbox.append(data_bbox[ti][i])\n",
    "    # Make slices start from 0\n",
    "    test_time.append(np.float32(t_time) - t_time[0])\n",
    "    test_bbox.append(np.float32(t_bbox))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3db8e5",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b09d2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, mu, sigma):\n",
    "    return (1.0 / np.sqrt(2 * np.pi * sigma**2)) * np.exp(-((x - mu)**2) / (2 * sigma**2))\n",
    "\n",
    "def soften(x, y, win_size, fn):\n",
    "    soft_y = []\n",
    "    for c in range(len(x)):\n",
    "        istart = max(c - win_size, 0)\n",
    "        iend = c + 1\n",
    "        # iend = min(c + win_size, len(x)-1)\n",
    "        weights = fn(x[istart:iend])\n",
    "        weights = weights / np.sum(weights)\n",
    "        soft_y.append(np.average(y[istart:iend], weights=weights, axis=0))\n",
    "    return np.float32(soft_y)\n",
    "\n",
    "def mse(x_t, x_p, axis=None):\n",
    "    t_area = np.mean(x_t, axis=0)\n",
    "    t_area = (t_area[1, 0] - t_area[0, 0]) * (t_area[1, 1] - t_area[0, 1])\n",
    "    return np.mean((x_t - x_p)**2) / t_area\n",
    "\n",
    "def get_center_area(bbox):\n",
    "    cter = np.sqrt(np.sum((0.5 * np.sum(bbox, axis=1))**2, axis=-1))\n",
    "    area = (bbox[:, 1, 0] - bbox[:, 0, 0]) * (bbox[:, 1, 1] - bbox[:, 0, 1])\n",
    "    cter = cter - np.mean(cter)\n",
    "    area = area - np.mean(area)\n",
    "    return cter, area\n",
    "\n",
    "def get_error(ldiff):\n",
    "    udiff = -ldiff\n",
    "    ldiff[ldiff < 0] = 0\n",
    "    udiff[udiff < 0] = 0\n",
    "    return (ldiff, udiff)\n",
    "\n",
    "def select_times(in_times, in_period, out_period):\n",
    "    out_times = np.round(in_times / in_period).astype(int)\n",
    "    k = int(np.round(out_period / in_period))\n",
    "    out_times = np.mod(out_times, k)\n",
    "    return np.argwhere(out_times == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c48601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the softnening strategy with the lowest error.\n",
    "test_cases = {\n",
    "    'Uniform': lambda x: np.ones_like(x),\n",
    "    'Linear': lambda x: 1.0 / (np.abs(x - x[-1]) + 1.0),\n",
    "    'Gaussian': lambda x: gaussian(x, x[-1], 1.0 / REFERENCE_FRAME_RATE),\n",
    "}\n",
    "\n",
    "soft_bbox = {k: [] for k in test_cases}\n",
    "mse_errors = {k: [] for k in test_cases}\n",
    "\n",
    "for label, op in test_cases.items():\n",
    "    for true_time, true_bbox in zip(test_time, test_bbox): \n",
    "        soft_bbox[label].append(soften(true_time, true_bbox, 5, op))\n",
    "        \n",
    "        mse_err = mse(true_bbox, soft_bbox[label][-1])\n",
    "        mse_errors[label].append(mse_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23fc7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = ((label, mse_err) for label, mse_errs in mse_errors.items() for mse_err in mse_errs)\n",
    "mse_df = pd.DataFrame(table, columns=['weight_type', 'mse'])\n",
    "mse_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8bdeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_df.groupby('weight_type').agg(np.sum).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc4c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_idx = {\n",
    "    'Min Error': np.argmin(mse_errors['Gaussian']),\n",
    "    'Med Error': np.argsort(mse_errors['Gaussian'])[len(mse_errors['Gaussian'])//2],\n",
    "    'Max Error': np.argmax(mse_errors['Gaussian']),\n",
    "}\n",
    "\n",
    "fig, axs = plt.subplots(nrows=len(example_idx),\n",
    "                        ncols=2,\n",
    "                        figsize=(4*len(example_idx), 12),\n",
    "                        constrained_layout=True,\n",
    "                        squeeze=False)\n",
    "\n",
    "line_labels = {}\n",
    "for i, (name, eidx) in enumerate(example_idx.items()):\n",
    "    cter_ax = axs[i, 0]\n",
    "    area_ax = axs[i, 1]\n",
    "    \n",
    "    true_time = test_time[eidx]\n",
    "    true_bbox = test_bbox[eidx]\n",
    "\n",
    "    true_cter, true_area = get_center_area(true_bbox)\n",
    "    l = cter_ax.scatter(true_time, true_cter, marker='o', label='Data')\n",
    "    l = area_ax.scatter(true_time, true_area, marker='o', label='Data')\n",
    "    line_labels['Data'] = l\n",
    "\n",
    "    for (label, op), m in zip(test_cases.items(), 'x*+'):\n",
    "        soft_cter, soft_area = get_center_area(soft_bbox[label][eidx])\n",
    "        mse_err = mse_errors[label][eidx]\n",
    "        \n",
    "        print(name + '\\t' + label, mse_err)\n",
    "        \n",
    "        l = cter_ax.scatter(true_time, soft_cter, marker=m)\n",
    "        l = area_ax.scatter(true_time, soft_area, marker=m)\n",
    "        line_labels[label] = l\n",
    "\n",
    "    cter_ax.set_title(name)\n",
    "    area_ax.set_title(name)\n",
    "\n",
    "labels, lines = zip(*line_labels.items())\n",
    "\n",
    "fig.set_constrained_layout(False)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.suptitle('Effect of smoothing on BBox Center and Area', fontsize=16)\n",
    "fig.legend(lines, labels, loc=('lower center'), ncol=len(labels))    \n",
    "fig.subplots_adjust(bottom=0.05, top=0.925)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d3117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_type = ['zero', 'slinear', 'quadratic', 'cubic']\n",
    "frame_rates = [15, 6, 3]\n",
    "soft_kind = 'Gaussian'\n",
    "\n",
    "table = []\n",
    "for true_time, true_bbox, sref_bbox in zip(test_time, test_bbox, soft_bbox[soft_kind]):\n",
    "    for interp_kind in interp_type:\n",
    "        for frame_rate in frame_rates:\n",
    "            idx = select_times(true_time, 1.0 / REFERENCE_FRAME_RATE, 1.0 / frame_rate)\n",
    "            \n",
    "            # Create the interpolated model from the subsampled data\n",
    "            sub_time = true_time[idx].flatten()\n",
    "            sub_bbox = true_bbox[idx].reshape(-1, 2, 2)\n",
    "            \n",
    "            try:\n",
    "                bbox_inter = interp1d(sub_time,\n",
    "                                      sub_bbox,\n",
    "                                      kind=interp_kind,\n",
    "                                      bounds_error=False,\n",
    "                                      fill_value='extrapolate',\n",
    "                                      axis=0)\n",
    "                fill_bbox = bbox_inter(true_time)\n",
    "            except ValueError as e:\n",
    "                fill_bbox = np.zeros_like(true_bbox)\n",
    "            \n",
    "            # Compare the interpolated points with the reference ones\n",
    "            mse_err = mse(sref_bbox, fill_bbox)\n",
    "            \n",
    "            table.append((interp_kind, frame_rate, mse_err))\n",
    "\n",
    "interp_df = pd.DataFrame(table, columns=['interpolation_type', 'frame_rate', 'MSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8887f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
